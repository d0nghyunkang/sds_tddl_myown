{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "w7_Catdev_Class.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mI2dGR6HVT3",
        "colab_type": "text"
      },
      "source": [
        "## Category Development, Weak Supervision and CLassification [test]\n",
        "Today we are gonna practice the Discovery and Exploration steps involved in a Content Analysis project. \n",
        "And finally we are gonna look at the Implementation of baseline text classifiers as described in the [\"baseline_classification.ipynb\"](https://github.com/ulfaslak/sds_tddl_2020/blob/master/baseline_classification.ipynb) notebook. \n",
        "\n",
        "Overall you will learn how to\n",
        "- setup a HSBM topic model within a docker environment (HSBM is unfortunatly hard to install)\n",
        "- Practice Computer Assisted Query Building - [\"Computer-Assisted Keyword and Document Set Discovery from Unstructured Text\"](https://gking.harvard.edu/publications/computer-assisted-keyword-and-document-set-discovery-fromunstructured-text) and [\"Building a Twitter opinion lexicon from automatically-annotated tweets\"](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwi4vabi56PoAhWEjqQKHUalAzEQFjAAegQIAxAB&url=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS095070511630106X&usg=AOvVaw11N9i8bUc2fwbbq9vLKsLY)\n",
        "- Practice Weak Supervision techniques combining lexical appraoches with NLP parsing systems (Stanfordnlp).\n",
        "\n",
        "\n",
        "Todays exercise will use the Kaggle Toxicity Classification dataset: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data\n",
        "- Follow the url. Sign in and download the zip file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9GNcX8VHVT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "import pandas as pd\n",
        "path2tox_data = 'jigsaw-unintended-bias-in-toxicity-classification/train.csv'\n",
        "df = pd.read_csv(path2tox_data)\n",
        "# subsample data to allow faster prototyping\n",
        "df = df.sample(5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OYUicuUHVT9",
        "colab_type": "code",
        "colab": {},
        "outputId": "07cdc396-4cee-4a33-9324-c75c929a5fbc"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>severe_toxicity</th>\n",
              "      <th>obscene</th>\n",
              "      <th>identity_attack</th>\n",
              "      <th>insult</th>\n",
              "      <th>threat</th>\n",
              "      <th>asian</th>\n",
              "      <th>atheist</th>\n",
              "      <th>...</th>\n",
              "      <th>article_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>funny</th>\n",
              "      <th>wow</th>\n",
              "      <th>sad</th>\n",
              "      <th>likes</th>\n",
              "      <th>disagree</th>\n",
              "      <th>sexual_explicit</th>\n",
              "      <th>identity_annotator_count</th>\n",
              "      <th>toxicity_annotator_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1732208</th>\n",
              "      <td>6246168</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>Mostly agree.  No authentic \"cowgirl\" would ev...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>394506</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321106</th>\n",
              "      <td>635727</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Operational knowledge does not require a billi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>153272</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>654</th>\n",
              "      <td>240885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>I understand completely- I was also affected b...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>32634</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1226258</th>\n",
              "      <td>5613454</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Why do they do this when mango season is over?...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>356626</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601897</th>\n",
              "      <td>978733</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>That was proved false many years ago. I could ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>167349</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 45 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id    target                                       comment_text  \\\n",
              "1732208  6246168  0.166667  Mostly agree.  No authentic \"cowgirl\" would ev...   \n",
              "321106    635727  0.000000  Operational knowledge does not require a billi...   \n",
              "654       240885  0.000000  I understand completely- I was also affected b...   \n",
              "1226258  5613454  0.000000  Why do they do this when mango season is over?...   \n",
              "601897    978733  0.000000  That was proved false many years ago. I could ...   \n",
              "\n",
              "         severe_toxicity  obscene  identity_attack    insult  threat  asian  \\\n",
              "1732208              0.0      0.0              0.0  0.166667     0.0    NaN   \n",
              "321106               0.0      0.0              0.0  0.000000     0.0    NaN   \n",
              "654                  0.0      0.0              0.0  0.000000     0.0    NaN   \n",
              "1226258              0.0      0.0              0.0  0.000000     0.0    NaN   \n",
              "601897               0.0      0.0              0.0  0.000000     0.0    0.0   \n",
              "\n",
              "         atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
              "1732208      NaN  ...      394506  approved      0    0    0      0         0   \n",
              "321106       NaN  ...      153272  approved      0    0    0      1         0   \n",
              "654          NaN  ...       32634  approved      0    0    0      0         0   \n",
              "1226258      NaN  ...      356626  approved      0    0    0      0         0   \n",
              "601897       0.0  ...      167349  approved      0    0    0      3         0   \n",
              "\n",
              "         sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
              "1732208              0.0                         0                         6  \n",
              "321106               0.0                         0                         4  \n",
              "654                  0.0                         0                         4  \n",
              "1226258              0.0                         0                         4  \n",
              "601897               0.0                         4                         4  \n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcOpRsR6HVUA",
        "colab_type": "text"
      },
      "source": [
        "# Discovery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f68VDrGcHVUA",
        "colab_type": "text"
      },
      "source": [
        "## Setting up HSBM\n",
        "HSBM is based on the Network Package graph-tool which unfortunatly is notoriously hard to install on your ordinary laptops. \n",
        "For those of you using Linux, you can try installing on your own computer using similar commands as instructed in the Google Cloud example. \n",
        "\n",
        "Instead we have two possibilties: \n",
        "1. Create a local \"server\" on your own computer using Docker.\n",
        "2. Use a Google Cloud server as introduced in Week 1 (see exercise 1 on how to setup Google Cloud)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XFHjxFrHVUB",
        "colab_type": "text"
      },
      "source": [
        "### Docker solution\n",
        "#### First Install docker (https://docs.docker.com/install/)\n",
        "#### Enable file sharing.\n",
        "Docker runs an operating system in a closed environment, so you have to create a port to share files. \n",
        "\n",
        "Go to the Docker settings: Right click the docker icon in press settings.\n",
        "Press the /Ressources tab.\n",
        "Then /File sharing tab.\n",
        "Select the drive to be shared.\n",
        "\n",
        "### Setup the Graph-tool docker image used for HSBM.\n",
        "#### pull the image.\n",
        "`docker pull tiagopeixoto/graph-tool`\n",
        "\n",
        "#### Run the image while mounting the drive to be shared. # first allow shared drives in settings of the docker app\n",
        "#### run this.\n",
        "`docker run -v c:\\:/mnt/c -p 8888:8888 -p 6006:6006  --name graphtool -it -u root -w /home/root tiagopeixoto/graph-tool bash`\n",
        "##### In this example it was a c\\: directory that I shared. c:\\  specified the path to the shared directory on the host (your computer), and /mnt/c refered to the path in the Docker container.\n",
        " \n",
        "#### Run Jupyter notebook and navigate to port localhost://8888 in your browser\n",
        "`jupyter notebook --ip 0.0.0.0 --allow-root`\n",
        "\n",
        "#### When using it the second time use the following commands, to simple start and attach to the container.\n",
        "`docker container start graphtool`\n",
        "\n",
        "`docker attach graphtool`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEfct4krHVUC",
        "colab_type": "text"
      },
      "source": [
        "## Google Cloud Solution\n",
        "- Login to the server you set up in week 1. See these instructions for setting it up (https://course.fast.ai/start_gcp.html)\n",
        "- Run the following conda commands:\n",
        "    - ```conda config --add channels conda-forge\n",
        "conda config --add channels ostrokach-forge\n",
        "conda config --add channels pkgw-forge\n",
        "conda install gtk3 pygobject graph-tool cairo```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEt_Ika7HVUD",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 7.1: Running a HSBM topic model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYDjpkOgHVUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Open a New .ipynb and copy the following cell to download the TOPSBM tutorial created by the Authors of HSBM.\n",
        "import requests\n",
        "for filename in ['corpus.txt','titles.txt','sbmtm.py','TopSBM-tutorial.ipynb']:\n",
        "    url = 'https://raw.githubusercontent.com/martingerlach/hSBM_Topicmodel/master/%s'%filename\n",
        "    with open(filename,'w') as f:\n",
        "        f.write(requests.get(url).text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqjwHDnZHVUG",
        "colab_type": "text"
      },
      "source": [
        "Following the Tutorial, 'TopSBM-tutorial.ipynb' to test out the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOSwvXnzHVUH",
        "colab_type": "text"
      },
      "source": [
        "### 7.1.extra: Run the HSBM model on the Toxicity Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azTMsf_NHVUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sbmtm\n",
        "\n",
        "\"\"\" TO DO \"\"\"\n",
        "# prepare documents\n",
        "    # remove urls\n",
        "    # remove infrequent words."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "TyMUwGRDHVUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create model and graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "awDBGRkxHVUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_XIuQGtHVUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# investigate topics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDmutrjjHVUR",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 7.2: Computer-Assisted Category Development\n",
        "First you need to install the [Gensim package](https://radimrehurek.com/gensim/)\n",
        "\n",
        "The package implements a range of unsupervised text methods, including classic topic modelling like LDA, Wordembeding models like Word2Vec and GloVe,  and an implementation of one of the early Paragraph Embedder \"Doc2Cec\". The package furthermore comes with a set of pretrained models that can be used for wordbased similarity search.\n",
        "\n",
        "In this exercise you will practice using pretrained Word Embeddings for similarity search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSvfyQ9UHVUS",
        "colab_type": "text"
      },
      "source": [
        "- Load a model from the Gensim (if not installed `! conda install -c anaconda gensim`) [pretrained models](https://github.com/RaRe-Technologies/gensim-data)\n",
        "    - I suggest you use the `\"glove-wiki-gigaword-300\"` model.\n",
        "\n",
        "```## Load gensim model\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "glove = api.load(\"glove-wiki-gigaword-300\")```\n",
        "\n",
        "- Play around with the `most_similar()` function of the model.\n",
        "    - Try defining both the `positive=` and `negative=` argument of the function. \n",
        ">> [See here for docs](https://radimrehurek.com/gensim/models/keyedvectors.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZFTWI8eHVUT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4856cc63-e7d2-46ce-e771-85feba5344e8"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "glove = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CHdA1ywSVel",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "b885da1a-5790-4ec4-8ffe-6c1a7206fd80"
      },
      "source": [
        "# # Play around with the most_similar() function of the model.\n",
        "result = glove.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0])) \n",
        "for i in range(1,9):\n",
        "  print(\"{}: {:.4f}\".format(*result[i]))\n",
        "\n",
        "print('\\n')\n",
        "result = glove.most_similar(positive=['cello', 'orchestra'], negative=['daughter'])\n",
        "print(\"{}: {:.4f}\".format(*result[0])) \n",
        "for i in range(1,9):\n",
        "  print(\"{}: {:.4f}\".format(*result[i]))\n",
        "\n",
        "print('\\n')\n",
        "result = glove.most_similar(positive=['conductor', 'orchestra'], negative=['driver'])\n",
        "print(\"{}: {:.4f}\".format(*result[0])) \n",
        "for i in range(1,9):\n",
        "  print(\"{}: {:.4f}\".format(*result[i]))\n",
        "\n",
        "print('\\n')\n",
        "result = glove.most_similar(positive=['bed', 'sleep'], negative=['desk'])\n",
        "print(\"{}: {:.4f}\".format(*result[0])) \n",
        "for i in range(1,9):\n",
        "  print(\"{}: {:.4f}\".format(*result[i]))\n",
        "\n",
        "print('\\n')\n",
        "result = glove.most_similar(positive=['sound'], negative=['light','silence'])\n",
        "print(\"{}: {:.4f}\".format(*result[0])) \n",
        "for i in range(1,9):\n",
        "  print(\"{}: {:.4f}\".format(*result[i]))\n",
        "\n",
        "print('\\n')\n",
        "result = glove.most_similar(positive=['pencil', 'paper'], negative=['bread'])\n",
        "print(\"{}: {:.4f}\".format(*result[0])) \n",
        "for i in range(1,9):\n",
        "  print(\"{}: {:.4f}\".format(*result[i]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d7095d1e4fcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'king'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'man'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'glove' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI-_8xYHHVUV",
        "colab_type": "text"
      },
      "source": [
        "## Build a lexicon from scratch\n",
        "- Inspect 5-10 documents in the dataset to create an Initial list of positive and negative words, or come up with a topic yourself (e.g. politics).\n",
        "- Create an input function for evaluating new words.\n",
        "    - Use the model.get_similar(word) function to get candidate words.\n",
        "    - Run through the candidates.\n",
        "    - print the word to be evaluated.\n",
        "    - use the builtin method `input()` to get manual input.\n",
        "    - use the input to either save or discard candidate words.\n",
        "    - repeat process until you have >20 words.\n",
        "- Apply a lookup function to the dataset.\n",
        "    - First you tokenize the data using a tokenizer of choice. I suggest nltk.tokenize.casual.TweetTokenizer()\n",
        "    - create a function that loops through the words and counts matching with your lexicon.\n",
        "Extra: Train your own word2vec model on the Full toxicity dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV6N6W_XHVUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a lexicon from scratch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7S8md5XHVUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra: train your own word2vec model on the full toxicity dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uToNE4bAHVUb",
        "colab_type": "text"
      },
      "source": [
        "## 7.3: Weak supervision combining sentiment lexicon and dependency parser.\n",
        "This exercise we will create a weak supervision scheme, that combines the Vader sentiment analysis tool implemented in the nltk package, with a nlp parser to extract relationships - i.e. who is the negative sentiment directed against.\n",
        "\n",
        "First install the stanza package(`! pip install stanza`), formerly known as stanfordnlp.\n",
        "\n",
        "The package supports a [multitude of languages](https://stanfordnlp.github.io/stanza/models.html), but each model should be downloaded first using the `stanza.download()` function. Here we need to download the english one: `stanza.download('en')`.\n",
        "\n",
        "1.import nltk.sentiment and initialize the vader sentiment analyzer.\n",
        "\n",
        "2. Apply the vader sentiment analyzer extracting only the 'neg' value, and plot the results in relation to the columns: `['black','female','asian','homosexual_gay_or_lesbian']` as a barchart. The columns express the social groups involved/addressed in the comment. \n",
        "\n",
        "3. Make a column in the dataset that expresses whether 'woman' + relevant synonyms found using the Glove model, is mentioned in the text.\n",
        "\n",
        "4. Compute the precision and recall of the woman identifier in relation to the 'female' column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FC_puOmHVUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5xSLd0AHVUe",
        "colab_type": "text"
      },
      "source": [
        "## Stanfordnlp to match adjectives and women.\n",
        "\n",
        "5. Run the nlp pipeline by initializing the parser `stanza.Pipeline('en').\n",
        "\n",
        "6. Transform the Parse object to a list of sentences using the function `nlp_to_dict`.\n",
        "    - This will return a list of sentences, which by themselves are lsit of words including the parsed properties such as Word class.  \n",
        "\n",
        "7. Now we should get acquinted with the information involved. Count the most common lemmas of each wordclass(i.e. the 'upos' property), and print the top 10 words.\n",
        "\n",
        "8. Apply the `extract_adj_noun()` that returns a dataframe of noun-adjective pairs. \n",
        "\n",
        "9. Keep only rows where \"women\" + synonmyms is found in the noun column.\n",
        "\n",
        "10. Inspect the adjectives used, and compute which are significantly more common using the *Chi Squared measure*:\n",
        "\n",
        "$Chi^2$ express the difference between the co-occurence we observe and what we would have expected to see if two events independent, relative to the latter. I.e. how many times more prevalent is the co-occurrence we observe than what we would expect if they were independent.\n",
        "\n",
        "$X^{2}=\\sum_{i, j} \\frac{\\left(O_{i j}-E_{i j}\\right)^{2}}{E_{i j}}$\n",
        "where $i$ ranges over rows of the table, $j$ ranges over columns, $O_{i j}$ is the observed value for cell ($i,j$) and $E_ij$ is the expected value.\n",
        " \n",
        "\n",
        "11. Match the adjectives to two positive and negative word lists precompilled in the NLTK package. \n",
        "```\n",
        "from nltk.corpus import opinion_lexicon\n",
        "positive_w = set(opinion_lexicon.positive())\n",
        "negative_w = set(opinion_lexicon.negative())\n",
        "```\n",
        "12. Finally wrap this into a function, that returns true if both women+synonyms is present, and a negative word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kl0wKeJHVUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import stanza"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCmffF4IHVUh",
        "colab_type": "code",
        "colab": {},
        "outputId": "c99bf744-e07e-47ab-8d96-35501e9bcdb5"
      },
      "source": [
        "# 1\n",
        " \n",
        "# download English model\n",
        "# initialize English neural pipeline\n",
        "# apply to df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 18.5MB/s]                    \n",
            "2020-03-18 20:06:15 INFO: Downloading default packages for language: en (English)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/en/default.zip: 100%|██████████| 402M/402M [00:52<00:00, 7.61MB/s] \n",
            "2020-03-18 20:07:16 INFO: Finished downloading models and saved to /home/snorre/stanza_resources.\n",
            "2020-03-18 20:07:16 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | ewt       |\n",
            "| pos       | ewt       |\n",
            "| lemma     | ewt       |\n",
            "| depparse  | ewt       |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2020-03-18 20:07:17 INFO: Use device: cpu\n",
            "2020-03-18 20:07:17 INFO: Loading: tokenize\n",
            "2020-03-18 20:07:17 INFO: Loading: pos\n",
            "2020-03-18 20:07:18 INFO: Loading: lemma\n",
            "2020-03-18 20:07:18 INFO: Loading: depparse\n",
            "2020-03-18 20:07:19 INFO: Loading: ner\n",
            "2020-03-18 20:07:19 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "aXbYwy-WHVUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Helper function\"\"\"\n",
        "\n",
        "def nlp_to_dict(doc,keep_sentence=True):\n",
        "    dependencies = []\n",
        "\n",
        "    if keep_sentence == True:\n",
        "        for sent in doc.sentences:\n",
        "            #keys = [i for i in dir(sent.words[0]) if '_' !=i[0] and i!='misc']\n",
        "            temp = []\n",
        "            for word in sent.words:\n",
        "                d = word.to_dict()\n",
        "                #d = {key:getattr(word,key) for key in keys}\n",
        "                d['index'] = int(d['id'])\n",
        "                temp.append(d)\n",
        "            dependencies.append(temp)\n",
        "    else:\n",
        "        max_id = 0\n",
        "        for sent in doc.sentences:\n",
        "            keys = [i for i in dir(sent.words[0]) if '_' !=i[0] and i!='misc']\n",
        "            words = sent.words\n",
        "            \n",
        "            for word in words:\n",
        "                d = word.to_dict()\n",
        "                #d = {key:getattr(word,key) for key in keys}\n",
        "                d['index'] = int(d['id'])\n",
        "                dependencies.append(d)\n",
        "    return dependencies\n",
        "\n",
        "df['nlp_parse'] = df.nlp.apply(nlp_to_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "FdpTQmkWHVUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2\n",
        "\n",
        "# apply nlp_to_dict function\n",
        "\n",
        "# 3\n",
        "\n",
        "# count lemmas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "ByZiGdaRHVUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Helper functions\"\"\"\n",
        "\n",
        "def make_network(d):\n",
        "    i2d = {}\n",
        "    g = nx.Graph()\n",
        "    for num,l in enumerate(d): \n",
        "        for i in l:\n",
        "            id_ = '%d_%d'%(num,int(i['id']))\n",
        "            i2d[id_] = i\n",
        "            parent = i['head'] \n",
        "            \n",
        "            if parent==0:\n",
        "                continue\n",
        "            parent = '%d_%d'%(num,parent)\n",
        "            g.add_edge(id_,parent)\n",
        "    # make full network\n",
        "    for i in g:\n",
        "        for j in g[i]:\n",
        "            for k in g[i]:\n",
        "                g.add_edge(j,k)\n",
        "    return g,i2d\n",
        "\n",
        "\n",
        "def extract_adj_noun(d):\n",
        "    g,i2d = make_network(d)\n",
        "    temp = []\n",
        "    for i in i2d:\n",
        "        d = i2d[i]\n",
        "        type = d['upos']\n",
        "        text_a,lemma_a = d['text'],d['lemma']\n",
        "        if type=='ADJ':\n",
        "            if not i in g:\n",
        "                continue\n",
        "            for n in g[i]:\n",
        "                if i2d[n]['upos'] == 'NOUN':\n",
        "                    d = i2d[n]\n",
        "                    text,lemma = d['text'],d['lemma']\n",
        "                    temp.append({'adj_text':text_a,'adj_lemma':lemma_a,'noun_text':text,'noun_lemma':lemma})\n",
        "    return pd.DataFrame(temp)\n",
        "\n",
        "adjnoun_df = extract_adj_noun(sample.nlp_parse.values[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "glAvOIWOHVUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4: Apply the extract_adj_noun()\n",
        "\n",
        "# Use the resulting df and (5) keep only rows where \"women\" + synonmyms is found in the noun column.\n",
        "\n",
        "# 6: Inspect the adjectives used, and compute which are significantly more common using the Chi Squared measure\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT_4RDMMHVUs",
        "colab_type": "text"
      },
      "source": [
        "# 7.4 Compiling the lexicons. \n",
        "- Here you should follow the instructions from the lexical_methods.ipynb notebook of compilling and downloading the lexicons. \n",
        "- Alternatively you can download the precompilled lexicon functions that the notebook builds using the link supplied on absalon. \n",
        "- download the lexical_mining.py script that wraps around all individual lexicon functions.\n",
        "- import and appy the lexical_mining script using the `.lexical_mining()` function to the toxicity_dataset.\n",
        "- Compare the different variables in relation to the Groups: `['black','female','asian','homosexual_gay_or_lesbian']` and see if the different sentiment analytical methods agree on which group is recieving most hostility.\n",
        "    - Do this by computing the mean of the different sentiment variables: `['vader_neg','afinn_afinn','liu_negative_count','hedometer_happiness']` in relation to each group. \n",
        "    - Then compare the ranking made by each sentiment variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "3MMfZSfTHVUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lexical_mining as lex\n",
        "\n",
        "# your code"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}